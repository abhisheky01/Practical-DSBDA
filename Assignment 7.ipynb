{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e159d2ef",
   "metadata": {},
   "source": [
    "## In this assignment we don't use any dataset, we use text as a input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13dcbd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing is an exciting area.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "text = \"Natural language processing is an exciting area.\"\n",
    "print(sent_tokenize(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2096fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "print(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a63914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "showing off the stop words filtration.\"\"\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "filtered_sentence = []\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53396f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0437114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('exciting', 'JJ'), ('area', 'NN'), ('.', '.')]\n",
      "[('Huge', 'NNP'), ('budget', 'NN'), ('allocated', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "txt = \"Natural language processing is an exciting area. Huge budget have been allocated for t\"\n",
    "tokenized = sent_tokenize(txt)\n",
    "for i in tokenized:\n",
    "\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "    wordsList=[w for w in wordsList if not w in stop_words]\n",
    "    \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    print(tagged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fe4df12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job', 'Science', 'of', 'key', 'the', 'Data', '21st', 'learning', 'data', 'science', 'is', 'machine', 'century', 'for', 'sexiest'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk \n",
    "import math\n",
    "first_sentence = \"Data Science is the sexiest job of the 21st century\"\n",
    "second_sentence = \"machine learning is the key for data science\"\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")#join them to remove common duplicate words \n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6206edee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>Science</th>\n",
       "      <th>of</th>\n",
       "      <th>key</th>\n",
       "      <th>the</th>\n",
       "      <th>Data</th>\n",
       "      <th>21st</th>\n",
       "      <th>learning</th>\n",
       "      <th>data</th>\n",
       "      <th>science</th>\n",
       "      <th>is</th>\n",
       "      <th>machine</th>\n",
       "      <th>century</th>\n",
       "      <th>for</th>\n",
       "      <th>sexiest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job  Science  of  key  the  Data  21st  learning  data  science  is  \\\n",
       "0    1        1   1    0    2     1     1         0     0        0   1   \n",
       "1    0        0   0    1    1     0     0         1     1        1   1   \n",
       "\n",
       "   machine  century  for  sexiest  \n",
       "0        0        1    0        1  \n",
       "1        1        0    1        0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "for word in second_sentence: \n",
    "    wordDictB[word]+=1\n",
    "pd.DataFrame([wordDictA, wordDictB])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5266e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   job  Science   of    key    the  Data  21st  learning   data  science  \\\n",
      "0  0.1      0.1  0.1  0.000  0.200   0.1   0.1     0.000  0.000    0.000   \n",
      "1  0.0      0.0  0.0  0.125  0.125   0.0   0.0     0.125  0.125    0.125   \n",
      "\n",
      "      is  machine  century    for  sexiest  \n",
      "0  0.100    0.000      0.1  0.000      0.1  \n",
      "1  0.125    0.125      0.0  0.125      0.0  \n"
     ]
    }
   ],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence) \n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d33199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        job   Science        of       key       the      Data      21st  \\\n",
      "0  0.030103  0.030103  0.030103  0.000000  0.060206  0.030103  0.030103   \n",
      "1  0.000000  0.000000  0.000000  0.037629  0.037629  0.000000  0.000000   \n",
      "\n",
      "   learning      data   science        is   machine   century       for  \\\n",
      "0  0.000000  0.000000  0.000000  0.030103  0.000000  0.030103  0.000000   \n",
      "1  0.037629  0.037629  0.037629  0.037629  0.037629  0.000000  0.037629   \n",
      "\n",
      "    sexiest  \n",
      "0  0.030103  \n",
      "1  0.000000  \n"
     ]
    }
   ],
   "source": [
    "def computeIDF(docList): \n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0) \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "    return(idfDict)\n",
    "idfs=computeIDF([wordDictA,wordDictB])\n",
    "def computeTFIDF(tfBow,idfs):\n",
    "    tfidf={}\n",
    "    for word,val in tfBow.items():\n",
    "        tfidf[word]=val*idfs[word]\n",
    "    return(tfidf)\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "#putting it in a dataframe \n",
    "idf= pd.DataFrame([idfFirst, idfSecond])\n",
    "print(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce92e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
